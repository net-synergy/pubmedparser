#+TITLE: Pubmed parser
#+PROPERTY: header-args:sh :eval no
#+PROPERTY: header-args:bash :eval no :session *readme* :results none

Read XML files and pull out selected values.
Values to collect are determined by paths found in a [[#structure-file][structure file]].
The structure file also includes a key which associates the values with a parent element and names, which determine which file to place the elements in.

Files can be passed as either gzipped or uncompressed XML files or from standard in.

For more info on Pubmed's XML files see: [[https://dtd.nlm.nih.gov/ncbi/pubmed/doc/out/190101/index.html][pubmed_190101.dtd.]]

Usage:
#+begin_src sh :eval no
  xml_read --cache-dir=cache --structure-file=structure.yml \
      data/*.xml.gz
#+end_src

The cache directory is where the results are stored.

Build:
Requires ~zlib~
#+begin_src sh :eval no
  make all
#+end_src

* Structure file
:PROPERTIES:
:CUSTOM_ID: structure-file
:header_args: eval no
:END:

The structure file is a YAML file containing key-value pairs for different tags and paths.
There are four required keys: ~root~, ~key~, ~key_features~, and ~nodes~.
~Root~ provide the top-level tag, in the case of the pubmed files this will be ~PubmedArticleSet~.

#+begin_src sh :tangle ./example/structure.yml
  root: "PubmedArticleSet"
#+end_src

Only tags below the root tag will be considered.

~Key~ is a reference tag.
In the pubmed case all data is with respect to a publication, so the key should identify the publication the values are linked to.
The ~PMID~ tag is a suitable candidate.

#+begin_src sh :tangle ./example/structure.yml
  key: {
    Publication: "/PubmedArticle/MedlineCitation/PMID"
  }
#+end_src

Unlike ~root~, the value of ~key~ should be a map.
In the example above, ~key~ is named publication and it's value is an XML path.
The name is used to determine the file to write ~PMID~ values to.
This file acts as a reference for all publications seen, even those for which no other value was found.
Key's value is a path indicating where to find the desired tag in the XML hierarchy.
All paths start after the root.

Both ~key_features~ and ~nodes~ determine the other values to collect.
The ~read_xml~ program does not distinguish between them, but it is useful for post-processing the data if we keep them separate (see [[#example-creating-node-and-edges]]).
~Key_features~ is for values that are apart of the key's node.
For instance, the date of publication is specific to the publication, all publications have one, and only one, publication date.
~Nodes~ handles other values that characterize a publication but are can have any number (including 0) of instances, such as references and authors.
These make up distinct node type (i.e. a given author is a node of type author which can be connected to various other nodes such as the publications that author contributed to).

#+begin_src sh :tangle ./example/structure.yml
  key_features: {
    Year: "/PubmedArticle/MedlineCitation/Article/Journal/JournalIssue/PubDate/Year",
    Language: "/PubmedArticle/MedlineCitation/Article/Language"
  }

  nodes: {
    Author: "/PubmedArticle/MedlineCitation/Article/AuthorList/Author/{LastName,ForeName}",
    Grant: "/PubmedArticle/MedlineCitation/Article/GrantList/Grant/GrantID",
    Chemical: "/PubmedArticle/MedlineCitation/ChemicalList/Chemical/NameOfSubstance/@UI",
    Qualifier: "/PubmedArticle/MedlineCitation/MeshHeadingList/MeshHeading/QualifierName/@UI",
    Descriptor: "/PubmedArticle/MedlineCitation/MeshHeadingList/MeshHeading/DescriptorName/@UI",
    Reference: "/PubmedArticle/PubmedData/ReferenceList/Reference/ArticleIdList/ArticleId/[@IdType='pubmed']"
  }
#+end_src

In the above example there is some special syntax.
For ~author~ multiple values are returned each time the ~author~ tag is found.
This is expressed by using the ~{child_tag1,child_tag2,...}~ notation.
Each child tag under the parent tag is collected and returned in a single row, represented multiple node features.
This notation prevents mismatches between features of a given node if any tags are missing.
In the case a tag is missing an empty string is printed.

To get an attribute from a tag, the ~@attribute~ notation is used---as in the ~chemical~ node.
Whenever a path ends with an attribute ~xml_read~ will collect both the attribute and tag value.

Lastly, ~[@attribute='value']~ filters for attributes with the supplied values.
In the case of ~reference~ above this will ensure all references are identified by ~PMID~ as opposed to other IDs, such as DOI, so ~references~ can be linked with ~publication~ nodes.

This example structure file can be found in the example directory of this project at: [[file:./example/structure.yml]]

* Example: creating node and edge files for importing with neo4j
:PROPERTIES:
:CUSTOM_ID: example-creating-node-and-edges
:END:

The resulting values can be used to generate node and edge files that work with ~neo4j~.
First step is to set up the environment:

#+begin_src bash :exports none :tangle ./example/env.sh
  #! /usr/bin/env bash
  # Setup variables for file paths and running code related to
  # generating neo4j db.
#+end_src

#+begin_src bash :tangle ./example/env.sh :eval no
  export top_dir=../$(dirname $0)
  export bin_dir=$top_dir/bin
  export cache_dir=~/Documents/david/data/synergy/cache
  export data_dir=~/Documents/david/data/pubmed
  export structure_file=$top_dir/example/structure.yml
  export import_dir=~/Documents/david/data/synergy/import
  export delete_cache=true # If true clear cache.
  export nthreads=64
#+end_src

#+begin_src bash :exports none :tangle ./example/read_xml_files.sh :eval no
  #! /usr/bin/env bash
  # Read all files in \$data_dir using READ_XML.

  source ./env.sh
#+end_src

#+begin_src bash :tangle ./example/read_xml_files.sh :eval no
  $delete_cache && \
      [ -d $cache_dir ] && \
      rm -r $cache_dir

  [ -d $import_dir ] && rm -r $import_dir
  mkdir -p $import_dir
#+end_src

#+begin_src bash :exports none
  top_dir=$PWD
  bin_dir=$top_dir/bin
  cache_dir=./cache
  data_dir=~/data/pubmed
  structure_file=$top_dir/example/structure.yml
  import_dir=~/data/synergy/import
  nthreads=4
  delete_cache=false # If true clear cache.

  $delete_cache && \
      [ -d $cache_dir ] && \
      rm -r $cache_dir && \
      mkdir -p $cache_dir

  rm -r $import_dir && mkdir -p $import_dir
#+end_src

Within the cache directory is a ~processed.txt~ file which contains a list of all files that have been read.
We can use this to filter out files that have already been read in the case the program has been stopped:

#+begin_src bash :eval no :tangle ./example/read_xml_files.sh
  files="$data_dir/*.xml.gz"
#+end_src

#+begin_src bash :exports none
  files="$data_dir/pubmed21n0001.xml.gz"
#+end_src

Then read the files.

#+begin_src bash :tangle ./example/read_xml_files.sh
  # Assuming the executables are in this directory and not installed globally.
  echo "Reading XML files..."

  PATH="$bin_dir:$PATH" OMP_NUM_THREADS="$nthreads" read_xml \
      --structure-file=$structure_file \
      --cache-dir=$cache_dir \
      $files

  echo "Finished reading XML files."
#+end_src

Now using the ~yaml_get_key_components~ executable, we can read the contents of the structure file into shell and use those to generate ids, create a list of all nodes for each node type, join the nodes with ~key~ to create edge lists, and generate ~neo4j~ style headers.

First, the ~yaml_get_key_components~ can be written into a wrapper function so we don't have to keep passing the same arguments.

#+begin_src bash :tangle ./example/env.sh :results none
  components() {
      local name=$1
      PATH="$bin_dir:$PATH" yaml_get_key_component \
          --structure-file=$structure_file $name
  }

  key_value=$(components key)
  export key=${key_value%%:*}
#+end_src

#+begin_src bash :exports none :tangle ./example/convert_for_neo4j.sh :eval no
  #!/usr/bin/env bash
  # Convert outputs from READ_XML_FILES to node and edge files for
  # importing to neo4j.

  source ./env.sh
#+end_src

** Generating node files
To generate nodes we will remove remove the ~PMID~ column, sort, keep the unique rows, then print with line numbers (which will be used as IDs).

#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  tabsep="=+=t=+=" # Key to keep non-id columns together
  spcsep="=+s+="

  gen_node() {
      local node=$1

      node_file=$cache_dir/$node.tsv
      paste <(cut -f1 $node_file) \
          <(cut -f1 --complement $node_file | \
          sed -e "s/\\t/$tabsep/g" -e  "s/\\s/$spcsep/g") | \
          sort -k 2 > \
          tmp_${node} && mv tmp_${node} $node_file

      cut -f1 --complement $node_file | sort -u | \
          cat -n | sed 's/^\s*//' > $import_dir/${node}_nodes.tsv
  }

  echo "Generating node files..."

  while IFS=': ' read node value; do
      [[ $node == "Reference" ]] && continue
      gen_node $node &
  done <<< "$(components nodes)"
  wait
#+end_src

*Note*: the ~reference~ file is really an edge file (publication--publication edges) so it doesn't make sense to make a node file for it.
Additionally, since both of it's columns are PMIDs they should not be renumbered, instead IDs generated for the ~publication~ nodes will be used to renumber both columns of ~reference~.

The ~key_features~ files will be added to the ~key~ file as another node file but that will be done later to prevent them from getting in the way of joins while creating the edge files.
But before making the edge files, we'll generate IDs for the key file too.
~PMID~ would work as a unique identifier, but by creating a IDs we can ensure the values are a range from 1--~n_publications~ which will make it possible to use as indices for other applications.

*Note*: The publications may reference files outside of the database leading to values in the second column of the reference file to not be included in the publication node file so we have to add them to the cache publication file.

#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  cat <(cut -f1 $cache_dir/$key.tsv) <(cut -f2 $cache_dir/Reference.tsv) \
      | sort -u | cat -n | sed 's/^\s*//' > $import_dir/${key}_nodes.tsv

  echo "Finished generating node files."
#+end_src

The resulting file has the new IDs in column one and the PMIDs in column two.
** Generating edge files
Now we need to join the cache files with their node files to add the node ID columns to the cache files.

#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  echo "Generating edge files..."

  while IFS=': ' read node value; do
      [[ $node == "Reference" ]] && continue
      join -j 2 $cache_dir/${node}.tsv $import_dir/${node}_nodes.tsv | \
          sort -k 2b,2 > $cache_dir/${node}_tmp.tsv &
  done <<< "$(components nodes)"
  wait
#+end_src

Then to create the edge files, join the publication node file with the cached files on PMID and remove the features so only the IDs are left.

#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  while IFS=': ' read node value; do
      [[ $node == "Reference" ]] && continue
      join -j 2 $cache_dir/${node}_tmp.tsv $import_dir/${key}_nodes.tsv | \
          awk '{ print $4,"\t",$3 }'> \
          $import_dir/${key}_${node}_edges.tsv &
  done <<< "$(components nodes)"
  wait
#+end_src

Now replace the PMIDs in the reference file to the new publication IDs by both columns with the publication nodes file.

#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  paste <(join -1 2 -2 1 $import_dir/${key}_nodes.tsv <(sort -k 1 $cache_dir/Reference.tsv) | cut -d" " -f2) \
      <(join -j 2 $import_dir/${key}_nodes.tsv <(sort -k 2 $cache_dir/Reference.tsv) | cut -d" " -f2) > \
      $import_dir/${key}_${key}_edges.tsv
#+end_src

** Joining key's features to key
#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  awk '{ print $2,$1 }' < $import_dir/${key}_nodes.tsv > tmp && \
      mv tmp $import_dir/${key}_nodes.tsv
  while IFS=': ' read key_feature value; do
      join -j 1 $import_dir/${key}_nodes.tsv \
          <(sort -k 1b,1 $cache_dir/${key_feature}.tsv) > tmp \
          && mv tmp $import_dir/${key}_nodes.tsv
  done <<< "$(components key_features)"
  sed 's/\s/\t/g' < $import_dir/${key}_nodes.tsv | cut -f 2- > tmp && \
      mv tmp $import_dir/${key}_nodes.tsv

  echo "Finished generating edge files."
#+end_src

** Cleaning up (replacing temporary separators)
#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  while IFS=': ' read node value; do
      [[ $node == "Reference" ]] && continue
      sed -e 's/ /\t/g' -e "s/$tabsep/\t/g" -e "s/$spcsep/ /g" < $import_dir/${node}_nodes.tsv > tmp_${node} && \
          mv tmp_${node} $import_dir/${node}_nodes.tsv &
  done <<< "$(components nodes)"
  wait
#+end_src

** Adding headers
For the key nodes:
#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  header="${key}Id:ID($key)"
  while IFS=': ' read node value; do
      header="${header}\t${node}"
  done <<< "$(components key_features)"

  cat <(echo -e $header) $import_dir/${key}_nodes.tsv > \
      tmp && mv tmp $import_dir/${key}_nodes.tsv
#+end_src

For other nodes:
#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  while IFS=': ' read node value; do
      [[ $node == "Reference" ]] && continue
      header="${node}Id:ID(${node})"
      IFS=','; for v in $value; do
          header="${header}\t${v}"
      done

      cat <(echo -e $header) $import_dir/${node}_nodes.tsv > \
          tmp && mv tmp $import_dir/${node}_nodes.tsv
  done <<< "$(components nodes)"
#+end_src

For edges (excluding references):
#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  while IFS=': ' read node value; do
      [[ $node == "Reference" ]] && continue
      header=":START_ID(${key})"
      header="${header}\t:END_ID(${node})"

      cat <(echo -e $header) $import_dir/${key}_${node}_edges.tsv > \
          tmp && mv tmp $import_dir/${key}_${node}_edges.tsv
  done <<< "$(components nodes)"
#+end_src

For the special case of references, both IDs should be publications:
#+begin_src bash :tangle ./example/convert_for_neo4j.sh
  header=":START_ID($key)\t:END_ID($key)"
  cat <(echo -e $header) $import_dir/${key}_${key}_edges.tsv > \
      tmp && mv tmp $import_dir/${key}_${key}_edges.tsv
#+end_src

* Example: importing with neo4j
#+begin_src bash :exports none :tangle ./example/generate_importer.sh :eval no
  #!/usr/bin/env bash
  # Create a neo4j importer for files in \$import_dir, defined in
  # env.sh.

  source ./env.sh
#+end_src

# TODO: generate based on files in $import_dir instead of all nodes
# since I may not be calculating overlap for mesh terms.
#+begin_src bash :tangle ./example/generate_importer.sh
  import_file=example/importpubmed.sh
  cat > $import_file <<_EOF_
  #!/usr/bin/env bash

  database_dir=\$XDG_DATA_HOME/neo4j/data
  import_dir=$import_dir
  name=neo4j

  [ -d \$database_dir ] && rm -r \$database_dir

  neo4j-admin import \\
      --database=\$name \\
      --delimiter="\\t" \\
      --quote="\\"" \\
      --skip-bad-relationships=true \\
      --trim-strings=true \\
      --id-type=STRING \\
  _EOF_

  while IFS=': ' read node value; do
      [[ $node == "Reference" ]] && continue
      echo "    --nodes=${node}=\$import_dir/${node}_nodes.tsv \\" >> $import_file
      echo "    --relationships=${key}-${node}=\$import_dir/${key}_${node}_edges.tsv \\" >> $import_file
  done <<< "$(components nodes)"

  echo "    --nodes=${key}=\$import_dir/${key}_nodes.tsv \\" >> $import_file
  echo "    --relationships=${key}-${key}=\$import_dir/${key}_${key}_edges.tsv \\" >> $import_file

  chmod +x $import_file
#+end_src
